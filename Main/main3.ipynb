{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VpDpjIJIQ9MQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data-input.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "oxO2sAcHSvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "b5ZiwDeoTK3Q"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mHcaT0eFUNe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found how to do most of this from \"https://realpython.com/python-data-cleaning-numpy-pandas/\"\n",
        "\n",
        "Just removing some of the columns that we dont really care about or have overlap with other columns."
      ],
      "metadata": {
        "id": "jBJjNlPiuvyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = ['keywords',\n",
        "            'homepage',\n",
        "            'production_countries',\n",
        "            'spoken_languages',\n",
        "            'tagline',\n",
        "            'status',\n",
        "            'original_title']\n",
        "\n",
        "new_names =  {'original_language': 'language'}\n",
        "df.drop(to_drop, inplace=True, axis=1)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "\n",
        "# There was a bunch of rows that had 0 runtime/revenue that were skewing all my data. hopefully this helps?\n",
        "df = df[df.revenue != 0]\n",
        "df = df[df.runtime != 0]\n",
        "df = df[df.vote_count != 0]\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "orF5J2zKUN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "VHXjQV-FZC6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# took from here https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas\n",
        "# getting year month day for future calculations potentially\n",
        "import datetime\n",
        "\n",
        "df['year'] = pd.DatetimeIndex(df['release_date']).year\n",
        "df['month'] = pd.DatetimeIndex(df['release_date']).month\n",
        "df['day'] = pd.DatetimeIndex(df['release_date']).day\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uHHFP8DZy_j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I had a value in my csv in 1927 with a 93 million dollar budget. I looked it up and this was incorrect\n",
        "# So I needed to clean this datapoint and remove it from the csv\n",
        "df = df[df.year != 1927]"
      ],
      "metadata": {
        "id": "vj01OBMG380P"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data-cleaned.csv')"
      ],
      "metadata": {
        "id": "wnBu4tZSZYRP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv(r'data-cleaned.csv')"
      ],
      "metadata": {
        "id": "h90j4iNZnJNN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1 = sns.scatterplot(x=\"budget\", y=\"revenue\", data=data)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# wouldnt save the jpg when I did it same as below for some reason.\n",
        "fig = plot1.get_figure()\n",
        "fig.savefig(\"plot1.jpg\")"
      ],
      "metadata": {
        "id": "WVqSQJla_cmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2 = sns.lineplot(x=\"year\", y=\"budget\", data=data)\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot2.jpg\")"
      ],
      "metadata": {
        "id": "gx2wFbfD7WrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot3 = sns.displot(data['month'])\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot3.jpg\")"
      ],
      "metadata": {
        "id": "Vx2Yq43N7aSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "-td9MDbliEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "# I dont think this is right but im dropping the columns I dont need now just so I can split the input/output values\n",
        "\n",
        "to_drop = ['Unnamed: 0',\n",
        "            'genres',\n",
        "            'id',\n",
        "            'language',\n",
        "            'overview',\n",
        "            'production_companies',\n",
        "            'release_date',\n",
        "            'title']\n",
        "\n",
        "data.drop(to_drop, inplace=True, axis=1)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "-TUHtNXJo4HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://stackoverflow.com/questions/43697240/how-can-i-split-a-dataset-from-a-csv-file-for-training-and-testing\n",
        "train = data.sample(frac=0.8, random_state = np.random.RandomState())\n",
        "test = data.loc[~data.index.isin(train.index)]\n",
        "\n",
        "train.to_csv('main_train.csv', index=False)\n",
        "test.to_csv('main_test.csv', index=False)\n",
        "\n",
        "print(\"--Get data--\")\n",
        "y_test = test.month\n",
        "x_test = test.drop('month', axis=1)\n",
        "\n",
        "y_train = train.month\n",
        "x_train = train.drop('month', axis=1)\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXRsX9b6knNG",
        "outputId": "d7342550-8d29-4461-cb73-0947bb354d65"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Get data--\n",
            "(2698, 8) (674, 8) (2698,) (674,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = tf.keras.layers.Normalization()\n",
        "normalize.adapt(x_train)"
      ],
      "metadata": {
        "id": "W8gxETJQHR9n"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Make model--\")\n",
        "model = tf.keras.models.Sequential([\n",
        "  normalize,\n",
        "  tf.keras.layers.Dense(64, input_shape=(8,), activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(13, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "  initial_learning_rate, decay_steps=100000, decay_rate=0.9, staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(x_train, y_train, epochs=50, verbose=2, batch_size = 20)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss1, model_acc1 = model.evaluate(x_train,  y_train, verbose=2)\n",
        "model_loss2, model_acc2 = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR0j51f8nST-",
        "outputId": "0f8f325d-d960-43bc-acc9-b22a2fca9d83"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Make model--\n",
            "--Fit model--\n",
            "Epoch 1/50\n",
            "135/135 - 1s - loss: 2.5284 - accuracy: 0.1175 - 778ms/epoch - 6ms/step\n",
            "Epoch 2/50\n",
            "135/135 - 0s - loss: 2.4223 - accuracy: 0.1690 - 237ms/epoch - 2ms/step\n",
            "Epoch 3/50\n",
            "135/135 - 0s - loss: 2.3884 - accuracy: 0.1775 - 248ms/epoch - 2ms/step\n",
            "Epoch 4/50\n",
            "135/135 - 0s - loss: 2.3663 - accuracy: 0.1883 - 248ms/epoch - 2ms/step\n",
            "Epoch 5/50\n",
            "135/135 - 0s - loss: 2.3540 - accuracy: 0.1887 - 240ms/epoch - 2ms/step\n",
            "Epoch 6/50\n",
            "135/135 - 0s - loss: 2.3445 - accuracy: 0.1957 - 243ms/epoch - 2ms/step\n",
            "Epoch 7/50\n",
            "135/135 - 0s - loss: 2.3358 - accuracy: 0.1987 - 268ms/epoch - 2ms/step\n",
            "Epoch 8/50\n",
            "135/135 - 0s - loss: 2.3267 - accuracy: 0.2005 - 232ms/epoch - 2ms/step\n",
            "Epoch 9/50\n",
            "135/135 - 0s - loss: 2.3190 - accuracy: 0.2001 - 250ms/epoch - 2ms/step\n",
            "Epoch 10/50\n",
            "135/135 - 0s - loss: 2.3133 - accuracy: 0.1961 - 254ms/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "135/135 - 0s - loss: 2.3053 - accuracy: 0.1994 - 286ms/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "135/135 - 0s - loss: 2.3014 - accuracy: 0.2083 - 262ms/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "135/135 - 0s - loss: 2.2934 - accuracy: 0.2024 - 247ms/epoch - 2ms/step\n",
            "Epoch 14/50\n",
            "135/135 - 0s - loss: 2.2886 - accuracy: 0.2057 - 223ms/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "135/135 - 0s - loss: 2.2810 - accuracy: 0.2113 - 252ms/epoch - 2ms/step\n",
            "Epoch 16/50\n",
            "135/135 - 0s - loss: 2.2782 - accuracy: 0.2109 - 242ms/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "135/135 - 0s - loss: 2.2715 - accuracy: 0.2157 - 237ms/epoch - 2ms/step\n",
            "Epoch 18/50\n",
            "135/135 - 0s - loss: 2.2671 - accuracy: 0.2235 - 238ms/epoch - 2ms/step\n",
            "Epoch 19/50\n",
            "135/135 - 0s - loss: 2.2615 - accuracy: 0.2179 - 252ms/epoch - 2ms/step\n",
            "Epoch 20/50\n",
            "135/135 - 0s - loss: 2.2571 - accuracy: 0.2261 - 240ms/epoch - 2ms/step\n",
            "Epoch 21/50\n",
            "135/135 - 0s - loss: 2.2495 - accuracy: 0.2294 - 226ms/epoch - 2ms/step\n",
            "Epoch 22/50\n",
            "135/135 - 0s - loss: 2.2453 - accuracy: 0.2265 - 230ms/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "135/135 - 0s - loss: 2.2418 - accuracy: 0.2246 - 238ms/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "135/135 - 0s - loss: 2.2338 - accuracy: 0.2320 - 228ms/epoch - 2ms/step\n",
            "Epoch 25/50\n",
            "135/135 - 0s - loss: 2.2330 - accuracy: 0.2242 - 219ms/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "135/135 - 0s - loss: 2.2271 - accuracy: 0.2317 - 226ms/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "135/135 - 0s - loss: 2.2231 - accuracy: 0.2357 - 246ms/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "135/135 - 0s - loss: 2.2167 - accuracy: 0.2350 - 233ms/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "135/135 - 0s - loss: 2.2146 - accuracy: 0.2354 - 242ms/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "135/135 - 0s - loss: 2.2107 - accuracy: 0.2394 - 273ms/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "135/135 - 0s - loss: 2.2052 - accuracy: 0.2368 - 257ms/epoch - 2ms/step\n",
            "Epoch 32/50\n",
            "135/135 - 0s - loss: 2.2018 - accuracy: 0.2380 - 278ms/epoch - 2ms/step\n",
            "Epoch 33/50\n",
            "135/135 - 0s - loss: 2.1988 - accuracy: 0.2398 - 253ms/epoch - 2ms/step\n",
            "Epoch 34/50\n",
            "135/135 - 0s - loss: 2.1965 - accuracy: 0.2435 - 281ms/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "135/135 - 0s - loss: 2.1908 - accuracy: 0.2465 - 272ms/epoch - 2ms/step\n",
            "Epoch 36/50\n",
            "135/135 - 0s - loss: 2.1901 - accuracy: 0.2435 - 258ms/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "135/135 - 0s - loss: 2.1833 - accuracy: 0.2472 - 232ms/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "135/135 - 0s - loss: 2.1793 - accuracy: 0.2491 - 255ms/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "135/135 - 0s - loss: 2.1768 - accuracy: 0.2524 - 256ms/epoch - 2ms/step\n",
            "Epoch 40/50\n",
            "135/135 - 0s - loss: 2.1745 - accuracy: 0.2476 - 228ms/epoch - 2ms/step\n",
            "Epoch 41/50\n",
            "135/135 - 0s - loss: 2.1693 - accuracy: 0.2443 - 227ms/epoch - 2ms/step\n",
            "Epoch 42/50\n",
            "135/135 - 0s - loss: 2.1686 - accuracy: 0.2520 - 238ms/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "135/135 - 0s - loss: 2.1611 - accuracy: 0.2628 - 241ms/epoch - 2ms/step\n",
            "Epoch 44/50\n",
            "135/135 - 0s - loss: 2.1627 - accuracy: 0.2572 - 241ms/epoch - 2ms/step\n",
            "Epoch 45/50\n",
            "135/135 - 0s - loss: 2.1574 - accuracy: 0.2617 - 219ms/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "135/135 - 0s - loss: 2.1538 - accuracy: 0.2587 - 240ms/epoch - 2ms/step\n",
            "Epoch 47/50\n",
            "135/135 - 0s - loss: 2.1542 - accuracy: 0.2576 - 234ms/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "135/135 - 0s - loss: 2.1495 - accuracy: 0.2646 - 244ms/epoch - 2ms/step\n",
            "Epoch 49/50\n",
            "135/135 - 0s - loss: 2.1454 - accuracy: 0.2628 - 226ms/epoch - 2ms/step\n",
            "Epoch 50/50\n",
            "135/135 - 0s - loss: 2.1456 - accuracy: 0.2557 - 227ms/epoch - 2ms/step\n",
            "--Evaluate model--\n",
            "85/85 - 0s - loss: 2.1183 - accuracy: 0.2706 - 256ms/epoch - 3ms/step\n",
            "22/22 - 0s - loss: 2.3820 - accuracy: 0.1780 - 56ms/epoch - 3ms/step\n",
            "Train / Test Accuracy: 27.1% / 17.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vic2x10J7We"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}