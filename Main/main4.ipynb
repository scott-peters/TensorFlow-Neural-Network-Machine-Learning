{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VpDpjIJIQ9MQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data-input.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "oxO2sAcHSvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "b5ZiwDeoTK3Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mHcaT0eFUNe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found how to do most of this from \"https://realpython.com/python-data-cleaning-numpy-pandas/\"\n",
        "\n",
        "Just removing some of the columns that we dont really care about or have overlap with other columns."
      ],
      "metadata": {
        "id": "jBJjNlPiuvyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = ['keywords',\n",
        "            'homepage',\n",
        "            'production_countries',\n",
        "            'spoken_languages',\n",
        "            'tagline',\n",
        "            'status',\n",
        "            'original_title']\n",
        "\n",
        "new_names =  {'original_language': 'language'}\n",
        "df.drop(to_drop, inplace=True, axis=1)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "\n",
        "# There was a bunch of rows that had 0 runtime/revenue that were skewing all my data. hopefully this helps?\n",
        "df = df[df.revenue != 0]\n",
        "df = df[df.runtime != 0]\n",
        "df = df[df.vote_count != 0]\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "orF5J2zKUN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "VHXjQV-FZC6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# took from here https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas\n",
        "# getting year month day for future calculations potentially\n",
        "import datetime\n",
        "\n",
        "df['year'] = pd.DatetimeIndex(df['release_date']).year\n",
        "df['month'] = pd.DatetimeIndex(df['release_date']).month\n",
        "df['day'] = pd.DatetimeIndex(df['release_date']).day\n",
        "\n",
        "\n",
        "# Changing language so I can use this in my model\n",
        "# af = 0, cn = 1, da = 2, de = 3, el = 4, en = 5, es = 6, fa = 7, fr = 8, he = 9, hi = 10, \n",
        "# id = 11, is = 12, it = 13, ja = 14, ko = 15, nb = 16, nl = 17, no = 18, pl = 19, pt = 20, ro = 21, \n",
        "# ru = 22, te = 23, th = 24, vi = 25, xx = 26, zh = 27\n",
        "df['language'] = df['language'].map({'af' : 0, 'cn' : 1, 'da' : 2, 'de' : 3, 'el' : 4, 'en' : 5,\n",
        "                                     'es' : 6, 'fa' : 7, 'fr' : 8, 'he' : 9, 'hi' : 10, 'id' : 11,\n",
        "                                     'is' : 12, 'it' : 13, 'ja' : 14, 'ko' : 15, 'nb' : 16, 'nl' : 17,\n",
        "                                     'no' : 18, 'pl' : 19, 'pt' : 20, 'ro' : 21, 'ru' : 22, 'te' : 23,\n",
        "                                     'th' : 24, 'vi' : 25, 'xx' : 26, 'zh' : 27})\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uHHFP8DZy_j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I had a value in my csv in 1927 with a 93 million dollar budget. I looked it up and this was incorrect\n",
        "# So I needed to clean this datapoint and remove it from the csv\n",
        "df = df[df.year != 1927]"
      ],
      "metadata": {
        "id": "vj01OBMG380P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data-cleaned.csv')"
      ],
      "metadata": {
        "id": "wnBu4tZSZYRP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv(r'data-cleaned.csv')"
      ],
      "metadata": {
        "id": "h90j4iNZnJNN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1 = sns.scatterplot(x=\"budget\", y=\"revenue\", data=data)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# wouldnt save the jpg when I did it same as below for some reason.\n",
        "fig = plot1.get_figure()\n",
        "fig.savefig(\"plot1.jpg\")"
      ],
      "metadata": {
        "id": "WVqSQJla_cmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2 = sns.lineplot(x=\"year\", y=\"budget\", data=data)\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot2.jpg\")"
      ],
      "metadata": {
        "id": "gx2wFbfD7WrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot3 = sns.displot(data['month'])\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot3.jpg\")"
      ],
      "metadata": {
        "id": "Vx2Yq43N7aSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "-td9MDbliEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "# I dont think this is right but im dropping the columns I dont need now just so I can split the input/output values\n",
        "\n",
        "to_drop = ['Unnamed: 0',\n",
        "            'genres',\n",
        "            'id',\n",
        "            'overview',\n",
        "            'production_companies',\n",
        "            'release_date',\n",
        "            'title']\n",
        "\n",
        "data.drop(to_drop, inplace=True, axis=1)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "-TUHtNXJo4HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://stackoverflow.com/questions/43697240/how-can-i-split-a-dataset-from-a-csv-file-for-training-and-testing\n",
        "train = data.sample(frac=0.8, random_state = np.random.RandomState())\n",
        "test = data.loc[~data.index.isin(train.index)]\n",
        "\n",
        "train.to_csv('main_train.csv', index=False)\n",
        "test.to_csv('main_test.csv', index=False)\n",
        "\n",
        "print(\"--Get data--\")\n",
        "y_test = test.month\n",
        "x_test = test.drop('month', axis=1)\n",
        "\n",
        "y_train = train.month\n",
        "x_train = train.drop('month', axis=1)\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXRsX9b6knNG",
        "outputId": "8ed7e8f1-f5b6-4570-df3b-5e541c25c08a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Get data--\n",
            "(2698, 9) (674, 9) (2698,) (674,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = tf.keras.layers.Normalization()\n",
        "normalize.adapt(x_train)"
      ],
      "metadata": {
        "id": "W8gxETJQHR9n"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Make model--\")\n",
        "model = tf.keras.models.Sequential([\n",
        "  normalize,\n",
        "  tf.keras.layers.Dense(256, input_shape=(8,), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(13, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "  initial_learning_rate, decay_steps=100000, decay_rate=0.9, staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(x_train, y_train, epochs=50, verbose=2, batch_size = 20)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss1, model_acc1 = model.evaluate(x_train,  y_train, verbose=2)\n",
        "model_loss2, model_acc2 = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR0j51f8nST-",
        "outputId": "37416041-523f-4447-cab2-bd06c1a984dc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Make model--\n",
            "--Fit model--\n",
            "Epoch 1/50\n",
            "135/135 - 1s - loss: 2.5574 - accuracy: 0.1316 - 1s/epoch - 9ms/step\n",
            "Epoch 2/50\n",
            "135/135 - 0s - loss: 2.5086 - accuracy: 0.1457 - 494ms/epoch - 4ms/step\n",
            "Epoch 3/50\n",
            "135/135 - 1s - loss: 2.4810 - accuracy: 0.1638 - 550ms/epoch - 4ms/step\n",
            "Epoch 4/50\n",
            "135/135 - 1s - loss: 2.4705 - accuracy: 0.1679 - 514ms/epoch - 4ms/step\n",
            "Epoch 5/50\n",
            "135/135 - 1s - loss: 2.4496 - accuracy: 0.1720 - 561ms/epoch - 4ms/step\n",
            "Epoch 6/50\n",
            "135/135 - 1s - loss: 2.4470 - accuracy: 0.1635 - 535ms/epoch - 4ms/step\n",
            "Epoch 7/50\n",
            "135/135 - 1s - loss: 2.4383 - accuracy: 0.1646 - 547ms/epoch - 4ms/step\n",
            "Epoch 8/50\n",
            "135/135 - 1s - loss: 2.4324 - accuracy: 0.1805 - 514ms/epoch - 4ms/step\n",
            "Epoch 9/50\n",
            "135/135 - 0s - loss: 2.4362 - accuracy: 0.1727 - 476ms/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "135/135 - 0s - loss: 2.4206 - accuracy: 0.1898 - 462ms/epoch - 3ms/step\n",
            "Epoch 11/50\n",
            "135/135 - 1s - loss: 2.4197 - accuracy: 0.1723 - 502ms/epoch - 4ms/step\n",
            "Epoch 12/50\n",
            "135/135 - 0s - loss: 2.4196 - accuracy: 0.1790 - 474ms/epoch - 4ms/step\n",
            "Epoch 13/50\n",
            "135/135 - 0s - loss: 2.4139 - accuracy: 0.1861 - 473ms/epoch - 4ms/step\n",
            "Epoch 14/50\n",
            "135/135 - 1s - loss: 2.4103 - accuracy: 0.1883 - 521ms/epoch - 4ms/step\n",
            "Epoch 15/50\n",
            "135/135 - 1s - loss: 2.3978 - accuracy: 0.1872 - 518ms/epoch - 4ms/step\n",
            "Epoch 16/50\n",
            "135/135 - 1s - loss: 2.3970 - accuracy: 0.1883 - 528ms/epoch - 4ms/step\n",
            "Epoch 17/50\n",
            "135/135 - 1s - loss: 2.3950 - accuracy: 0.1875 - 587ms/epoch - 4ms/step\n",
            "Epoch 18/50\n",
            "135/135 - 1s - loss: 2.3966 - accuracy: 0.1846 - 531ms/epoch - 4ms/step\n",
            "Epoch 19/50\n",
            "135/135 - 1s - loss: 2.3924 - accuracy: 0.1931 - 516ms/epoch - 4ms/step\n",
            "Epoch 20/50\n",
            "135/135 - 1s - loss: 2.3963 - accuracy: 0.1883 - 506ms/epoch - 4ms/step\n",
            "Epoch 21/50\n",
            "135/135 - 0s - loss: 2.3824 - accuracy: 0.1898 - 461ms/epoch - 3ms/step\n",
            "Epoch 22/50\n",
            "135/135 - 0s - loss: 2.3845 - accuracy: 0.1868 - 462ms/epoch - 3ms/step\n",
            "Epoch 23/50\n",
            "135/135 - 0s - loss: 2.3706 - accuracy: 0.1913 - 490ms/epoch - 4ms/step\n",
            "Epoch 24/50\n",
            "135/135 - 1s - loss: 2.3729 - accuracy: 0.2005 - 503ms/epoch - 4ms/step\n",
            "Epoch 25/50\n",
            "135/135 - 1s - loss: 2.3674 - accuracy: 0.1931 - 569ms/epoch - 4ms/step\n",
            "Epoch 26/50\n",
            "135/135 - 1s - loss: 2.3788 - accuracy: 0.1935 - 511ms/epoch - 4ms/step\n",
            "Epoch 27/50\n",
            "135/135 - 0s - loss: 2.3647 - accuracy: 0.1905 - 465ms/epoch - 3ms/step\n",
            "Epoch 28/50\n",
            "135/135 - 0s - loss: 2.3656 - accuracy: 0.1901 - 463ms/epoch - 3ms/step\n",
            "Epoch 29/50\n",
            "135/135 - 0s - loss: 2.3545 - accuracy: 0.1953 - 499ms/epoch - 4ms/step\n",
            "Epoch 30/50\n",
            "135/135 - 0s - loss: 2.3589 - accuracy: 0.1964 - 476ms/epoch - 4ms/step\n",
            "Epoch 31/50\n",
            "135/135 - 0s - loss: 2.3505 - accuracy: 0.2027 - 494ms/epoch - 4ms/step\n",
            "Epoch 32/50\n",
            "135/135 - 0s - loss: 2.3571 - accuracy: 0.2020 - 483ms/epoch - 4ms/step\n",
            "Epoch 33/50\n",
            "135/135 - 0s - loss: 2.3413 - accuracy: 0.2039 - 476ms/epoch - 4ms/step\n",
            "Epoch 34/50\n",
            "135/135 - 0s - loss: 2.3416 - accuracy: 0.2027 - 492ms/epoch - 4ms/step\n",
            "Epoch 35/50\n",
            "135/135 - 1s - loss: 2.3417 - accuracy: 0.2079 - 520ms/epoch - 4ms/step\n",
            "Epoch 36/50\n",
            "135/135 - 1s - loss: 2.3371 - accuracy: 0.2076 - 507ms/epoch - 4ms/step\n",
            "Epoch 37/50\n",
            "135/135 - 1s - loss: 2.3491 - accuracy: 0.2057 - 505ms/epoch - 4ms/step\n",
            "Epoch 38/50\n",
            "135/135 - 0s - loss: 2.3373 - accuracy: 0.2090 - 469ms/epoch - 3ms/step\n",
            "Epoch 39/50\n",
            "135/135 - 0s - loss: 2.3325 - accuracy: 0.2061 - 487ms/epoch - 4ms/step\n",
            "Epoch 40/50\n",
            "135/135 - 0s - loss: 2.3382 - accuracy: 0.2105 - 456ms/epoch - 3ms/step\n",
            "Epoch 41/50\n",
            "135/135 - 0s - loss: 2.3221 - accuracy: 0.2024 - 480ms/epoch - 4ms/step\n",
            "Epoch 42/50\n",
            "135/135 - 0s - loss: 2.3335 - accuracy: 0.2035 - 458ms/epoch - 3ms/step\n",
            "Epoch 43/50\n",
            "135/135 - 0s - loss: 2.3346 - accuracy: 0.2191 - 484ms/epoch - 4ms/step\n",
            "Epoch 44/50\n",
            "135/135 - 0s - loss: 2.3188 - accuracy: 0.2124 - 449ms/epoch - 3ms/step\n",
            "Epoch 45/50\n",
            "135/135 - 0s - loss: 2.3125 - accuracy: 0.2120 - 463ms/epoch - 3ms/step\n",
            "Epoch 46/50\n",
            "135/135 - 0s - loss: 2.3210 - accuracy: 0.2165 - 448ms/epoch - 3ms/step\n",
            "Epoch 47/50\n",
            "135/135 - 0s - loss: 2.3095 - accuracy: 0.2228 - 463ms/epoch - 3ms/step\n",
            "Epoch 48/50\n",
            "135/135 - 0s - loss: 2.3145 - accuracy: 0.2079 - 495ms/epoch - 4ms/step\n",
            "Epoch 49/50\n",
            "135/135 - 0s - loss: 2.2972 - accuracy: 0.2146 - 470ms/epoch - 3ms/step\n",
            "Epoch 50/50\n",
            "135/135 - 0s - loss: 2.3099 - accuracy: 0.2150 - 465ms/epoch - 3ms/step\n",
            "--Evaluate model--\n",
            "85/85 - 0s - loss: 2.2075 - accuracy: 0.2635 - 335ms/epoch - 4ms/step\n",
            "22/22 - 0s - loss: 2.4012 - accuracy: 0.2062 - 54ms/epoch - 2ms/step\n",
            "Train / Test Accuracy: 26.4% / 20.6%\n"
          ]
        }
      ]
    }
  ]
}