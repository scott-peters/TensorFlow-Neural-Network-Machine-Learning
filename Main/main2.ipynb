{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDpjIJIQ9MQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data-input.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "oxO2sAcHSvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "b5ZiwDeoTK3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mHcaT0eFUNe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found how to do most of this from \"https://realpython.com/python-data-cleaning-numpy-pandas/\"\n",
        "\n",
        "Just removing some of the columns that we dont really care about or have overlap with other columns."
      ],
      "metadata": {
        "id": "jBJjNlPiuvyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = ['keywords',\n",
        "            'homepage',\n",
        "            'production_countries',\n",
        "            'spoken_languages',\n",
        "            'tagline',\n",
        "            'status',\n",
        "            'original_title']\n",
        "\n",
        "new_names =  {'original_language': 'language'}\n",
        "df.drop(to_drop, inplace=True, axis=1)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "\n",
        "# There was a bunch of rows that had 0 runtime/revenue that were skewing all my data. hopefully this helps?\n",
        "df = df[df.revenue != 0]\n",
        "df = df[df.runtime != 0]\n",
        "df = df[df.vote_count != 0]\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "orF5J2zKUN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "VHXjQV-FZC6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# took from here https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas\n",
        "# getting year month day for future calculations potentially\n",
        "import datetime\n",
        "\n",
        "df['year'] = pd.DatetimeIndex(df['release_date']).year\n",
        "df['month'] = pd.DatetimeIndex(df['release_date']).month\n",
        "df['day'] = pd.DatetimeIndex(df['release_date']).day\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uHHFP8DZy_j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I had a value in my csv in 1927 with a 93 million dollar budget. I looked it up and this was incorrect\n",
        "# So I needed to clean this datapoint and remove it from the csv\n",
        "df = df[df.year != 1927]"
      ],
      "metadata": {
        "id": "vj01OBMG380P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data-cleaned.csv')"
      ],
      "metadata": {
        "id": "wnBu4tZSZYRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv(r'data-cleaned.csv')"
      ],
      "metadata": {
        "id": "h90j4iNZnJNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1 = sns.scatterplot(x=\"budget\", y=\"revenue\", data=data)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# wouldnt save the jpg when I did it same as below for some reason.\n",
        "fig = plot1.get_figure()\n",
        "fig.savefig(\"plot1.jpg\")"
      ],
      "metadata": {
        "id": "WVqSQJla_cmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2 = sns.lineplot(x=\"year\", y=\"budget\", data=data)\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot2.jpg\")"
      ],
      "metadata": {
        "id": "gx2wFbfD7WrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot3 = sns.displot(data['month'])\n",
        "\n",
        "plt.show\n",
        "\n",
        "plt.savefig(\"plot3.jpg\")"
      ],
      "metadata": {
        "id": "Vx2Yq43N7aSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "-td9MDbliEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "# I dont think this is right but im dropping the columns I dont need now just so I can split the input/output values\n",
        "\n",
        "to_drop = ['Unnamed: 0',\n",
        "            'genres',\n",
        "            'id',\n",
        "            'language',\n",
        "            'overview',\n",
        "            'production_companies',\n",
        "            'release_date',\n",
        "            'title']\n",
        "\n",
        "data.drop(to_drop, inplace=True, axis=1)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "-TUHtNXJo4HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://stackoverflow.com/questions/43697240/how-can-i-split-a-dataset-from-a-csv-file-for-training-and-testing\n",
        "train = data.sample(frac=0.8, random_state = np.random.RandomState())\n",
        "test = data.loc[~data.index.isin(train.index)]\n",
        "\n",
        "train.to_csv('main_train.csv', index=False)\n",
        "test.to_csv('main_test.csv', index=False)\n",
        "\n",
        "print(\"--Get data--\")\n",
        "y_test = test.month\n",
        "x_test = test.drop('month', axis=1)\n",
        "\n",
        "y_train = train.month\n",
        "x_train = train.drop('month', axis=1)\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXRsX9b6knNG",
        "outputId": "35365d3f-ab49-40ff-8162-3074873d134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Get data--\n",
            "(2698, 8) (674, 8) (2698,) (674,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.describe()"
      ],
      "metadata": {
        "id": "0vHI_pbdWCYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Make model--\")\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(64, input_shape=(8,), activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(13, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "  initial_learning_rate, decay_steps=100000, decay_rate=0.9, staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(x_train, y_train, epochs=50, verbose=2, batch_size = 20)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss1, model_acc1 = model.evaluate(x_train,  y_train, verbose=2)\n",
        "model_loss2, model_acc2 = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR0j51f8nST-",
        "outputId": "842abdc1-8cec-4870-de91-0b8913827b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Make model--\n",
            "--Fit model--\n",
            "Epoch 1/50\n",
            "135/135 - 1s - loss: 3788165.7500 - accuracy: 0.0875 - 1s/epoch - 9ms/step\n",
            "Epoch 2/50\n",
            "135/135 - 0s - loss: 741695.1875 - accuracy: 0.0856 - 285ms/epoch - 2ms/step\n",
            "Epoch 3/50\n",
            "135/135 - 0s - loss: 571967.3750 - accuracy: 0.0886 - 276ms/epoch - 2ms/step\n",
            "Epoch 4/50\n",
            "135/135 - 0s - loss: 547517.5000 - accuracy: 0.0967 - 279ms/epoch - 2ms/step\n",
            "Epoch 5/50\n",
            "135/135 - 0s - loss: 601266.0625 - accuracy: 0.0823 - 280ms/epoch - 2ms/step\n",
            "Epoch 6/50\n",
            "135/135 - 0s - loss: 506958.8125 - accuracy: 0.0823 - 287ms/epoch - 2ms/step\n",
            "Epoch 7/50\n",
            "135/135 - 0s - loss: 597651.9375 - accuracy: 0.0908 - 292ms/epoch - 2ms/step\n",
            "Epoch 8/50\n",
            "135/135 - 0s - loss: 448102.8125 - accuracy: 0.0923 - 274ms/epoch - 2ms/step\n",
            "Epoch 9/50\n",
            "135/135 - 0s - loss: 534155.6875 - accuracy: 0.0878 - 279ms/epoch - 2ms/step\n",
            "Epoch 10/50\n",
            "135/135 - 0s - loss: 454312.8438 - accuracy: 0.0927 - 291ms/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "135/135 - 0s - loss: 469340.5938 - accuracy: 0.0860 - 264ms/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "135/135 - 0s - loss: 527649.6250 - accuracy: 0.0890 - 256ms/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "135/135 - 0s - loss: 487241.2500 - accuracy: 0.0997 - 252ms/epoch - 2ms/step\n",
            "Epoch 14/50\n",
            "135/135 - 0s - loss: 421858.1250 - accuracy: 0.1008 - 278ms/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "135/135 - 0s - loss: 353265.6250 - accuracy: 0.0804 - 282ms/epoch - 2ms/step\n",
            "Epoch 16/50\n",
            "135/135 - 0s - loss: 483056.9062 - accuracy: 0.0997 - 246ms/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "135/135 - 0s - loss: 446375.5312 - accuracy: 0.0808 - 281ms/epoch - 2ms/step\n",
            "Epoch 18/50\n",
            "135/135 - 0s - loss: 441659.7812 - accuracy: 0.0886 - 281ms/epoch - 2ms/step\n",
            "Epoch 19/50\n",
            "135/135 - 0s - loss: 345599.3438 - accuracy: 0.0938 - 246ms/epoch - 2ms/step\n",
            "Epoch 20/50\n",
            "135/135 - 0s - loss: 426187.6562 - accuracy: 0.0919 - 277ms/epoch - 2ms/step\n",
            "Epoch 21/50\n",
            "135/135 - 0s - loss: 327028.7812 - accuracy: 0.0886 - 260ms/epoch - 2ms/step\n",
            "Epoch 22/50\n",
            "135/135 - 0s - loss: 314850.7188 - accuracy: 0.0967 - 279ms/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "135/135 - 0s - loss: 362808.2812 - accuracy: 0.0949 - 267ms/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "135/135 - 0s - loss: 288802.5625 - accuracy: 0.0949 - 267ms/epoch - 2ms/step\n",
            "Epoch 25/50\n",
            "135/135 - 0s - loss: 361996.8125 - accuracy: 0.0993 - 269ms/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "135/135 - 0s - loss: 303756.4062 - accuracy: 0.0801 - 283ms/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "135/135 - 0s - loss: 276116.2188 - accuracy: 0.0897 - 289ms/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "135/135 - 0s - loss: 276912.2812 - accuracy: 0.0882 - 276ms/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "135/135 - 0s - loss: 245882.6562 - accuracy: 0.0827 - 293ms/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "135/135 - 0s - loss: 259982.4688 - accuracy: 0.0860 - 272ms/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "135/135 - 0s - loss: 261382.0469 - accuracy: 0.0856 - 280ms/epoch - 2ms/step\n",
            "Epoch 32/50\n",
            "135/135 - 0s - loss: 247120.7969 - accuracy: 0.0882 - 278ms/epoch - 2ms/step\n",
            "Epoch 33/50\n",
            "135/135 - 0s - loss: 267415.6250 - accuracy: 0.0890 - 274ms/epoch - 2ms/step\n",
            "Epoch 34/50\n",
            "135/135 - 0s - loss: 229514.9062 - accuracy: 0.0919 - 271ms/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "135/135 - 0s - loss: 234791.1562 - accuracy: 0.0949 - 279ms/epoch - 2ms/step\n",
            "Epoch 36/50\n",
            "135/135 - 0s - loss: 232287.6250 - accuracy: 0.0945 - 290ms/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "135/135 - 0s - loss: 203464.8906 - accuracy: 0.0949 - 269ms/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "135/135 - 0s - loss: 196639.5781 - accuracy: 0.0927 - 265ms/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "135/135 - 0s - loss: 191702.0312 - accuracy: 0.0953 - 280ms/epoch - 2ms/step\n",
            "Epoch 40/50\n",
            "135/135 - 0s - loss: 178237.1875 - accuracy: 0.1042 - 287ms/epoch - 2ms/step\n",
            "Epoch 41/50\n",
            "135/135 - 0s - loss: 195172.6562 - accuracy: 0.0915 - 268ms/epoch - 2ms/step\n",
            "Epoch 42/50\n",
            "135/135 - 0s - loss: 197245.2500 - accuracy: 0.0904 - 284ms/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "135/135 - 0s - loss: 158318.3594 - accuracy: 0.0949 - 276ms/epoch - 2ms/step\n",
            "Epoch 44/50\n",
            "135/135 - 0s - loss: 153756.5781 - accuracy: 0.0893 - 286ms/epoch - 2ms/step\n",
            "Epoch 45/50\n",
            "135/135 - 0s - loss: 155770.7969 - accuracy: 0.1019 - 268ms/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "135/135 - 0s - loss: 116954.1797 - accuracy: 0.0997 - 277ms/epoch - 2ms/step\n",
            "Epoch 47/50\n",
            "135/135 - 0s - loss: 145457.0156 - accuracy: 0.0867 - 314ms/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "135/135 - 0s - loss: 121812.3281 - accuracy: 0.0975 - 302ms/epoch - 2ms/step\n",
            "Epoch 49/50\n",
            "135/135 - 0s - loss: 160681.2188 - accuracy: 0.0882 - 278ms/epoch - 2ms/step\n",
            "Epoch 50/50\n",
            "135/135 - 0s - loss: 83845.5000 - accuracy: 0.0919 - 284ms/epoch - 2ms/step\n",
            "--Evaluate model--\n",
            "85/85 - 0s - loss: 11891.5391 - accuracy: 0.0915 - 277ms/epoch - 3ms/step\n",
            "22/22 - 0s - loss: 11194.5098 - accuracy: 0.1053 - 48ms/epoch - 2ms/step\n",
            "Train / Test Accuracy: 9.2% / 10.5%\n"
          ]
        }
      ]
    }
  ]
}